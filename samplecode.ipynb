{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ca676f",
   "metadata": {},
   "source": [
    "# \"Unlocking the Power of Data: Building Your Own Unique and High-Quality Datasets for Machine Learning and Data Science\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ed89c",
   "metadata": {},
   "source": [
    "Four different ways to acquire your own dataset, complete with Python code demonstrations.\n",
    "Have you ever wanted to work on a data science project, but couldn't find a dataset that suited your needs? Or perhaps you're looking to gain more experience in data collection and cleaning? In either case, building your own dataset can be a rewarding and informative experience. In this article, we'll cover four ways to acquire your own dataset and provide Python code examples for each method. And outlines of a few more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dcb429",
   "metadata": {},
   "source": [
    "###### 1. Web Scraping\n",
    "Web scraping involves extracting data from websites by automating the process of sending HTTP requests and parsing HTML responses. This method can be used to extract data from online databases, news articles, social media platforms, and other web-based sources. For example, you might scrape data from online marketplaces to build a dataset of product prices and descriptions.\n",
    "Here's an example of how to scrape data from a website using Python's requests and BeautifulSoup libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09adcd03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country / Area UN continental region[4] UN statistical subregion[4]  \\\n",
      "0          India                     Asia               Southern Asia   \n",
      "1       China[a]                     Asia                Eastern Asia   \n",
      "2  United States                 Americas            Northern America   \n",
      "3      Indonesia                     Asia          South-eastern Asia   \n",
      "4       Pakistan                     Asia               Southern Asia   \n",
      "\n",
      "   Population (1 July 2022)  Population (1 July 2023)  Change  \n",
      "0                1417173173                1428627663  +0.81%  \n",
      "1                1425887337                1425671352  −0.02%  \n",
      "2                 338289857                 339996564  +0.50%  \n",
      "3                 275501339                 277534123  +0.74%  \n",
      "4                 235824863                 240485658  +1.98%  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find_all('table')[0]\n",
    "\n",
    "data = pd.read_html(str(table))[0]\n",
    "data.to_csv('country_population.csv', index=False)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf2529",
   "metadata": {},
   "source": [
    "###### Detailed Code Explanation\n",
    "First, we import the necessary libraries: requests, pandas, and BeautifulSoup. requests is a library for making HTTP requests in Python, pandas is a library for data manipulation and analysis, and BeautifulSoup is a library for parsing HTML and XML documents.\n",
    "We then define the URL of the webpage we want to scrape using the url variable. In this example, we are scraping the Wikipedia page for a list of countries by population.\n",
    "Next, we use the requests.get() function to retrieve the webpage content as a response object. We store this response object in the response variable.\n",
    "We then create a BeautifulSoup object from the response content using the BeautifulSoup() function. We pass the response.content as the first argument and 'html.parser' as the second argument to specify that we want to parse an HTML document. We store this BeautifulSoup object in the soup variable.\n",
    "Next, we use the soup.find_all() method to find all the tables in the HTML document. We pass 'table' as the argument to specify that we want to find all the tables. We then select the first table in the list using [0] and store it in the table variable.\n",
    "We then use the pd.read_html() function to parse the HTML table and convert it to a Pandas dataframe. We pass the str(table)  as the argument to specify that we want to read the HTML table as a  string. We then select the first dataframe in the list using [0] and store it in the data variable.\n",
    "We save the data dataframe to a CSV file named 'country_population.csv' in the current working directory using the to_csv() method. We set the index argument to False to avoid writing the row indices to the CSV file.\n",
    "Finally, we print the first few rows of the data dataframe using the head() method to verify that the data was scraped and converted to a dataframe correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3f9f2",
   "metadata": {},
   "source": [
    "###### 2. Data Generation with \"Faker\"\n",
    "Data generation involves creating synthetic datasets from scratch or modifying existing datasets to include additional data points. This method can be useful for testing models, augmenting existing datasets, or simulating scenarios that are difficult to observe in real life. For example, you might generate a dataset of fake customer transactions to test a fraud detection algorithm.\n",
    "Here's an example of how to generate a dataset of random customer transactions using Python's random and faker libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942207cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID: 6898\n",
      "Timestamp: 2022-07-28 20:30:56\n",
      "Product: widget\n",
      "Price: 79.07\n",
      "\n",
      "Customer ID: 6605\n",
      "Timestamp: 2023-03-13 08:23:23\n",
      "Product: widget\n",
      "Price: 15.16\n",
      "\n",
      "Customer ID: 9863\n",
      "Timestamp: 2022-11-11 17:36:05\n",
      "Product: widget\n",
      "Price: 52.26\n",
      "\n",
      "Customer ID: 3051\n",
      "Timestamp: 2022-07-09 17:41:53\n",
      "Product: gizmo\n",
      "Price: 21.06\n",
      "\n",
      "Customer ID: 7757\n",
      "Timestamp: 2022-06-02 08:28:32\n",
      "Product: widget\n",
      "Price: 81.39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint, choice, random\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "transactions = []\n",
    "for i in range(5):\n",
    "    customer_id = randint(1000, 9999)\n",
    "    timestamp = faker.date_time_between(start_date='-1y', end_date='now').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    product = choice(['widget', 'gizmo', 'thingamajig'])\n",
    "    price = round(randint(1, 100) + random(), 2)\n",
    "    transactions.append({'Customer ID': customer_id, 'Timestamp': timestamp, 'Product': product, 'Price': price})\n",
    "\n",
    "for transaction in transactions:\n",
    "    print(f\"Customer ID: {transaction['Customer ID']}\\nTimestamp: {transaction['Timestamp']}\\nProduct: {transaction['Product']}\\nPrice: {transaction['Price']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0fa8b2",
   "metadata": {},
   "source": [
    "###### Detailed Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52cf02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, choice, random\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0536e0e",
   "metadata": {},
   "source": [
    "The first line imports three functions, randint, choice, and random from the random module. The randint function generates a random integer between two given numbers, choice returns a random element from a list, and random generates a random float between 0 and 1. The second line imports the Faker class from the faker module, which is used to generate fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd465fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "faker = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4872",
   "metadata": {},
   "source": [
    "This line creates an instance of the Faker class and assigns it to the variable faker. The Faker class provides methods for generating fake data in a variety of formats, including names, addresses, phone numbers, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d6cc0",
   "metadata": {},
   "source": [
    "This line creates an empty list called transactions, which will be used to store the generated transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdb705",
   "metadata": {},
   "source": [
    "This line starts a for loop that will run five times, generating five transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6306470",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = randint(1000, 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0a040",
   "metadata": {},
   "source": [
    "This line generates a random integer between 1000 and 9999 and assigns it to the variable customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = faker.date_time_between(start_date='-1y', end_date='now').strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db946d",
   "metadata": {},
   "source": [
    "This line generates a random timestamp between one year ago and the present using the Faker instance's date_time_between method. The resulting timestamp is formatted as a string with the %Y-%m-%d %H:%M:%S format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1d2e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = choice(['widget', 'gizmo', 'thingamajig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb78b37",
   "metadata": {},
   "source": [
    "This line randomly selects one of three products, widget, gizmo, or thingamajig, and assigns it to the variable product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf29f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = round(randint(1, 100) + random(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1d89b",
   "metadata": {},
   "source": [
    "This line generates a random price for the transaction by adding a random integer between 1 and 100 to a random float between 0 and 1, rounded to two decimal places, and assigns it to the variable price.\n",
    "transactions.append({'Customer ID': customer_id, 'Timestamp': timestamp, 'Product': product, 'Price': price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10c52477",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.append({'Customer ID': customer_id, 'Timestamp': timestamp, 'Product': product, 'Price': price})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689effb1",
   "metadata": {},
   "source": [
    "This line creates a dictionary containing the generated customer ID, timestamp, product, and price, and appends it to the transactions list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db6ffca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID: 6898\n",
      "Timestamp: 2022-07-28 20:30:56\n",
      "Product: widget\n",
      "Price: 79.07\n",
      "\n",
      "Customer ID: 6605\n",
      "Timestamp: 2023-03-13 08:23:23\n",
      "Product: widget\n",
      "Price: 15.16\n",
      "\n",
      "Customer ID: 9863\n",
      "Timestamp: 2022-11-11 17:36:05\n",
      "Product: widget\n",
      "Price: 52.26\n",
      "\n",
      "Customer ID: 3051\n",
      "Timestamp: 2022-07-09 17:41:53\n",
      "Product: gizmo\n",
      "Price: 21.06\n",
      "\n",
      "Customer ID: 7757\n",
      "Timestamp: 2022-06-02 08:28:32\n",
      "Product: widget\n",
      "Price: 81.39\n",
      "\n",
      "Customer ID: 7757\n",
      "Timestamp: 2022-06-02 08:28:32\n",
      "Product: thingamajig\n",
      "Price: 71.67\n",
      "\n",
      "Customer ID: 7757\n",
      "Timestamp: 2022-06-02 08:28:32\n",
      "Product: thingamajig\n",
      "Price: 71.67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for transaction in transactions:\n",
    "    print(f\"Customer ID: {transaction['Customer ID']}\\nTimestamp: {transaction['Timestamp']}\\nProduct: {transaction['Product']}\\nPrice: {transaction['Price']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f06837",
   "metadata": {},
   "source": [
    "Finally, this code loops over the transactions list and  prints out each transaction's customer ID, timestamp, product, and price  in a formatted string with newlines between each item. This creates a  user-friendly output of the generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17014b41",
   "metadata": {},
   "source": [
    "###### 3. Generating a Random Dataset Using NumPy and Pandas\n",
    "Here's an example of how to generate a simple dataset using the NumPy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f0ae6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Column 1  Column 2  Column 3  Column 4\n",
      "0  0.084719  0.616583 -0.402154 -0.475159\n",
      "1  0.448959 -2.281844 -0.024368 -1.499126\n",
      "2  0.892911  0.135883 -0.237427  1.478178\n",
      "3 -2.069098 -1.036908  0.598262  0.607105\n",
      "4 -0.903280  1.061673 -0.750074  0.504612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate random data\n",
    "data = np.random.randn(100, 4)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "df = pd.DataFrame(data, columns=['Column 1', 'Column 2', 'Column 3', 'Column 4'])\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv('my_dataset.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea9e54",
   "metadata": {},
   "source": [
    "###### Detailed Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd65c6d",
   "metadata": {},
   "source": [
    "###### First, \n",
    "we import the NumPy and Pandas libraries using the import  statement. NumPy is a library for numerical computing in Python, while  Pandas is a library for data manipulation and analysis. We then generate  a 100x4 NumPy array of random numbers using the np.random.randn() function. The randn() function generates an array of random numbers from the standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd203c",
   "metadata": {},
   "source": [
    "###### Next, \n",
    "we create a Pandas dataframe from the NumPy array using the pd.DataFrame() function. We pass the NumPy array as the first argument, and a list of column names as the columns argument. In this example, we have named the columns 'Column 1', 'Column 2', 'Column 3', and 'Column 4'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce852a1d",
   "metadata": {},
   "source": [
    "###### We save \n",
    "The Pandas dataframe to a CSV file using the to_csv() method. We pass the filename 'my_dataset.csv' as the first argument, and set the index argument to False to avoid writing the row indices to the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268642f",
   "metadata": {},
   "source": [
    "###### Finally, \n",
    "We print the first few rows of the Pandas dataframe using the head()  method, which returns the first 5 rows by default. This allows us to  verify that the dataset was generated correctly and to get a glimpse of  the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3ac51",
   "metadata": {},
   "source": [
    "###### 4. Data Synthesis\n",
    "This code loads two heart disease datasets from the UCI Machine Learning Repository, combines them vertically, cleans and transforms the data, and scales the continuous variables. Finally, it prints the first 5 rows of the synthesized dataset.\n",
    "The output will be a pandas DataFrame with 32 columns and an arbitrary number of rows, depending on how many rows were present in the original datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5125c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  sex  trestbps      chol  fbs  restecg   thalach  exang   oldpeak  \\\n",
      "0  1.302286  1.0  0.731945 -0.262961  1.0      2.0  0.233067    0.0  1.389362   \n",
      "1  1.743088  1.0  1.584739  0.640984  0.0      2.0 -1.533539    1.0  0.640255   \n",
      "2  1.743088  1.0 -0.689377 -0.331184  0.0      2.0 -0.650236    1.0  1.670278   \n",
      "3 -1.562928  1.0 -0.120848  0.026983  0.0      0.0  1.789364    0.0  2.513023   \n",
      "4 -1.122126  0.0 -0.120848 -0.757573  0.0      2.0  1.158433    0.0  0.546616   \n",
      "\n",
      "    ca  ...  cp_1.0  cp_2.0  cp_3.0  cp_4.0  slope_1.0  slope_2.0  slope_3.0  \\\n",
      "0  0.0  ...       1       0       0       0          0          0          1   \n",
      "1  3.0  ...       0       0       0       1          0          1          0   \n",
      "2  2.0  ...       0       0       0       1          0          1          0   \n",
      "3  0.0  ...       0       0       1       0          0          0          1   \n",
      "4  0.0  ...       0       1       0       0          1          0          0   \n",
      "\n",
      "   thal_3.0  thal_6.0  thal_7.0  \n",
      "0         0         1         0  \n",
      "1         1         0         0  \n",
      "2         0         0         1  \n",
      "3         1         0         0  \n",
      "4         1         0         0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the two datasets\n",
    "df1 = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header=None, na_values=\"?\")\n",
    "df2 = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\", header=None, na_values=\"?\")\n",
    "\n",
    "# Combine the two datasets vertically\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "\n",
    "# Replace missing values with the median of each column\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "cp_dummies = pd.get_dummies(df[\"cp\"], prefix=\"cp\")\n",
    "slope_dummies = pd.get_dummies(df[\"slope\"], prefix=\"slope\")\n",
    "thal_dummies = pd.get_dummies(df[\"thal\"], prefix=\"thal\")\n",
    "df = pd.concat([df, cp_dummies, slope_dummies, thal_dummies], axis=1)\n",
    "df.drop([\"cp\", \"slope\", \"thal\"], axis=1, inplace=True)\n",
    "\n",
    "# Scale continuous variables\n",
    "continuous_vars = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "df[continuous_vars] = (df[continuous_vars] - df[continuous_vars].mean()) / df[continuous_vars].std()\n",
    "\n",
    "# Show the first 5 rows of the synthesized dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3821b08",
   "metadata": {},
   "source": [
    "###### Detailed Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391b7a0",
   "metadata": {},
   "source": [
    "These are the standard imports for working with data in Python. Pandas is a popular library for working with tabular data, and NumPy is a library for working with numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41697f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two datasets\n",
    "df1 = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header=None, na_values=\"?\")\n",
    "df2 = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\", header=None, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83330a6",
   "metadata": {},
   "source": [
    "These lines load two datasets from the UCI Machine Learning Repository into Pandas dataframes. Both datasets are related to heart disease diagnosis, but they come from different sources and have different formats. The header=None argument tells Pandas that the files do not contain column names, and na_values=\"?\" specifies that missing data is represented by a question mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b4b0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets using column names as the key\n",
    "merged_df = pd.merge(df1, df2, on=[0,1,2,3,4,5,6,7,8,9,10,11,12], how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ac8ef",
   "metadata": {},
   "source": [
    "This line merges the two datasets into a single dataframe, using the column names as the key. The on parameter specifies the columns to merge on, and how=\"outer\" specifies that all rows from both datasets should be included in the merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "merged_df.columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca42334",
   "metadata": {},
   "source": [
    "This line renames the columns in the merged dataframe to more descriptive names, using the names provided in the original datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e39066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new synthetic data by adding random noise to the original dataset\n",
    "noise = np.random.normal(0, 0.1, size=merged_df.shape)\n",
    "synthetic_data = merged_df + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e1712",
   "metadata": {},
   "source": [
    "These lines create a new synthetic dataset by adding random noise to the merged dataset. The np.random.normal function generates random numbers with a normal distribution, with a mean of 0 and a standard deviation of 0.1. The size parameter specifies the dimensions of the noise array, which is the same as the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original and synthetic datasets\n",
    "concatenated_data = pd.concat([merged_df, synthetic_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce49292",
   "metadata": {},
   "source": [
    "This line concatenates the original merged dataset with the synthetic dataset, resulting in a larger dataset with twice as many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the concatenated dataset to a CSV file\n",
    "concatenated_data.to_csv(\"heart_disease_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b973440",
   "metadata": {},
   "outputs": [],
   "source": [
    "This line saves the concatenated dataset to a CSV file named \"heart_disease_data.csv\", without including the index column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e840d1",
   "metadata": {},
   "source": [
    "A few more different ways you can go about creating your own data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507e2ef",
   "metadata": {},
   "source": [
    "###### Data Annotation\n",
    "First up, we have data annotation. This is where you take an existing dataset and add additional information to it, such as labels or annotations, to make it more useful for a specific task. It's kind of like giving your dataset a makeover - you're taking something that's already pretty good and making it even better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfdfad",
   "metadata": {},
   "source": [
    "###### Manual Data Collection\n",
    "Next, we have manual data collection. This is where you go out and gather data yourself, whether it's through surveys, interviews, or observations. It can be a lot of work, but it can also be really rewarding, like a scavenger hunt where you're the one setting the rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3afc2",
   "metadata": {},
   "source": [
    "###### Data Augmentation\n",
    "Then there's data augmentation, which is like playing dress-up with your dataset. You take an existing dataset and generate new data by making small modifications to the existing data, such as flipping an image or rotating a 3D object. It's like giving your dataset a whole new wardrobe without having to go shopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11493949",
   "metadata": {},
   "source": [
    "###### Data Labeling\n",
    "Data labeling is another method, which involves assigning labels or tags to data points so that machine learning algorithms can learn from them. It's like putting name tags on all your friends at a party so that you can introduce them to someone new."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4656b7",
   "metadata": {},
   "source": [
    "###### Crowdsourcing\n",
    "If you don't feel like doing all the work yourself, you can always try crowdsourcing. This involves getting a group of people to help you collect or annotate data. It's like having your own personal army of data collectors, but without all the training and equipment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131740a",
   "metadata": {},
   "source": [
    "###### Data Fusion\n",
    "Finally, we have data fusion, which is like a superhero team-up for datasets. You take multiple datasets and merge them together to create a more complete and comprehensive dataset. It's like the Avengers, but for data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b30ff",
   "metadata": {},
   "source": [
    "So, there you have it - a few different ways you can create your own datasets. But, hey, if you want to learn more about coding and data science, why not subscribe to my blog and leave a comment asking for more examples? I promise it'll be fun and educational, like going on a treasure hunt for knowledge. So, what are you waiting for? Let's get coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02da84",
   "metadata": {},
   "source": [
    "###### Question for readers: \n",
    "Have you ever created your own dataset? What method did you use and how did it turn out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca29536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04494d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
